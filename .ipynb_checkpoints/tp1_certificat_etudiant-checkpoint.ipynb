{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à l'opimisation différentiable : méthodes de descente de gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but du TP est d'illustrer les performances d'un certain nombre de méthodes classiques pour l'optimisation sans contrainte, dans le cas de problèmes quadratiques et non-linéaires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Chargement des librairies\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "from scipy import linalg as sla\n",
    "import math\n",
    "import functools as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Définition des problèmes d'optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les problèmes étudiés seront les suivants :\n",
    " \n",
    "   $(\\mathcal{P}_1)\\quad \\min_{x \\in \\mathbb{R}^n} f(x)=x^TAx+b^Tx$, avec $A$ symétrique définie positive;\n",
    "   \n",
    "   $(\\mathcal{P}_2)\\quad \\min_{x \\in \\mathbb{R}^2} f(x)=100(x_2-x_1^2)^2+(1-x_1)^2$, avec f la fonction de Rosenbrock.\n",
    "  \n",
    "La minimisation de fonctionnelles quadratiques (problème $(\\mathcal{P}_1)$) constitue la base sur lequel s'appuie un certain nombre de méthodes d'optimisation. Le problème $(\\mathcal{P}_2)$ permet d'illustrer les difficultés qui surgissent quand on cherche à minimsier une fonction non-linéaire non-quadratique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Minimisation d'une fonction quadratique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problèmes quadratiques\n",
    "def build_problem(n,condA):\n",
    "    # Construction d'une matrice symétrique définie positive\n",
    "    # et d'un vecteur de même dimension\n",
    "    # n : entier, dimension de la matrice\n",
    "    # condA : entier, conditionnement de la matrice 10^condA\n",
    "    # Sortie : matrice A, vecteur b\n",
    "    \n",
    "    A = np.zeros([n,n])\n",
    "    Q,R=sla.qr(np.random.rand(n,n))\n",
    "    D=np.diag((10*np.ones([n]))**np.linspace(-condA,0,num=n))\n",
    "    A=np.matmul(np.matmul(np.transpose(Q),D),Q)    \n",
    "    \n",
    "    b=np.random.randn(n,1)\n",
    "    \n",
    "    return A,b\n",
    "\n",
    "# Paramètres du problème\n",
    "condA=3\n",
    "n=100\n",
    "A,b=build_problem(n,condA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Implanter les fonctions f, gradient de f et Hessienne de f associée au problème $(\\mathcal{P}_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fquad(x,A,b):\n",
    "    # Fonction quadratique : f(x)=0.5*x'*A*x+b'*x\n",
    "    # A : matrice symétrique de taille n\n",
    "    # b,x : vecteurs de taille n\n",
    "    \n",
    "    f = 0.5 * x.T.dot(A.dot(x)) + b.T.dot(x)\n",
    "    \n",
    "    return f\n",
    "\n",
    "def gquad(x,A,b):\n",
    "    # Gradient de la fonction fquad\n",
    "    # A : matrice symétrique de taille n\n",
    "    # b,x : vecteurs de taille n\n",
    "    \n",
    "    g = A.dot(x) + b\n",
    "    \n",
    "    return g\n",
    "    \n",
    "def hquad(x,A,b):\n",
    "    # Gradient de la fonction fquad\n",
    "    # A : matrice symétrique de taille n\n",
    "    # b,x : vecteurs de taille n\n",
    "    \n",
    "    H = A\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f =  [[14.]]\n",
      "G =  [[ 7.]\n",
      " [12.]]\n",
      "H =  [[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "A_test = np.matrix('1 2; 3 4')\n",
    "b_test = np.matrix('4; 5')\n",
    "x_test = np.ones((2,1))\n",
    "print(\"f = \", fquad(x_test,A_test,b_test))\n",
    "print('G = ', gquad(x_test, A_test, b_test))\n",
    "print('H = ', hquad(x_test, A_test, b_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### I-1 Application de la méthode de la plus grande pente (\"steepest descent\").\n",
    " \n",
    " **Question**: Implanter la méthode de la \"steepest descent\" pour la minimisation d'une fonction quadratique. Vous pourrez anticiper l'utilisation de méthode de descente de gradient pour le cas de fonctions non-quadratiques (different choix de pas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# Au choix : une fonction \"steepest descente\" ou des fonctions descentes et \"pas optimal\".\n",
    "# En sortie, la fonction devra renvoyer : \n",
    "#       x : vecteur contenant l'itéré optimisé.\n",
    "#       flag : un entier indiquant la raison d'arrêt de l'algorithme : 1 - convergence, \n",
    "#              2 - stagnation de la suite des itérés, 3 - nombre d'itérations maximum atteint.\n",
    "#       nbiter : nombre d'itérations réalisées.\n",
    "#       iterf : tableau contenant les valeurs de f au cours de la minimisation.\n",
    "\n",
    "def steepest_descent(x0, eps, max_iter, A, b):\n",
    "    \"\"\"steepest_descent\n",
    "    @param x0 : variable initiale\n",
    "    @param eps : tolérance, contains 2 elements\n",
    "    @param max_iter : nombre iteration maximale\n",
    "    \"\"\"\n",
    "    iterf = np.zeros(max_iter)    \n",
    "    x = x0\n",
    "    flag = -1\n",
    "    for k in range(max_iter):\n",
    "        d = - gquad(x, A, b)\n",
    "        alpha = d.T.dot(d) / d.T.dot(A.dot(d))\n",
    "        x_last = x\n",
    "        x = x + alpha * d\n",
    "        iterf[k] = fquad(x, A, b)\n",
    "        \n",
    "        # convergency : gradient = 0\n",
    "        if la.norm( gquad(x, A, b) ) < eps[0] *( la.norm( gquad(x0, A, b) ) + 10**-10) :\n",
    "            flag = 1\n",
    "            break\n",
    "            \n",
    "        # stagnation : gradient descent slowly\n",
    "        if la.norm( x - x_last ) <= eps[1] * (la.norm(x) + 10**-10) :\n",
    "            flag = 2\n",
    "            break\n",
    "    \n",
    "    nbiter = k   \n",
    "    if nbiter == max_iter -1 :\n",
    "        flag = 3\n",
    "    else:\n",
    "        iterf = iterf[0:nbiter]\n",
    "    \n",
    "    return x, flag, nbiter, iterf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Résoudre le problème d'optimisation $(\\mathcal{P}_1)$. Evaluez la sensibilité des résultats aux paramètres d'entrée de l'algorithme ($x_0$, tolérances, nombre d'itérations maximum,..)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeur optimale théorique de f: [[-8859.06244552]]\n",
      "Valeur à l'optinum numérique: [[-8859.06244192]]\n",
      "Condition d'arrêt: 1\n",
      "[ -312.82435485  -510.02696938  -633.04165399 ... -8859.06244188\n",
      " -8859.06244189 -8859.06244191]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Définition des paramètres de la méthode : x0, tolérances, nombre d'itérations,etc ..\n",
    "x0 = np.random.rand(n,1)\n",
    "tolérances = np.array([10**-5, 10**-10])\n",
    "nombre_iteration = 10000\n",
    "\n",
    "# Résolution du problème d'optimisation\n",
    "x, flag, nbiter, iterf = steepest_descent(x0, tolérances, nombre_iteration, A, b)\n",
    "\n",
    "# Affichages \n",
    "#  x est le vecteur obtenu en sortie de minimisation.\n",
    "#  flag est l'entier indiquant la condition d'arrêt activée. \n",
    "#  fopt est la valeur optimale théorique.\n",
    "#  iterf est le tableau contenant les valeurs de f au cours de la minimisation.\n",
    "\n",
    "tmp=sla.solve(A,-b)\n",
    "fopt=0.5*np.dot(np.transpose(b),tmp)\n",
    "print(\"Valeur optimale théorique de f:\",fopt)\n",
    "print(\"Valeur à l'optinum numérique:\", fquad(x,A,b))\n",
    "print(\"Condition d'arrêt:\", flag)\n",
    "if flag > 1:\n",
    "    print(\"Norme du gradient initial:\",la.norm(gquad(x0,A,b)))\n",
    "    print(\"Norme du gradient:\",la.norm(gquad(x,A,b)))\n",
    "\n",
    "print(iterf)\n",
    "# plt.plot(iterf[:,0,0])\n",
    "plt.plot(iterf[:])\n",
    "plt.ylabel('f')\n",
    "plt.xlabel('Itérations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Illuster l'impact du conditionnement de $A$ sur les performances de l'algorithme. Vous afficherez l'évolution du nombre d'itérations en focntion du conitionnement de $A$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intitialisation du problème\n",
    "x0=np.random.randn(n,1)\n",
    "param=[10**(-6),10**(-12),100000]\n",
    "iterC=np.array([])\n",
    "condAmax=6\n",
    "\n",
    "# Boucle sur le nombre de conditionnement\n",
    "for condA in range(condAmax):\n",
    "    # Génération et résolution du problème d'optimisation\n",
    "    x, flag, nbiter, iterf = steepest_descent(x0, param[0:-1], param[-1], condA, )\n",
    "    \n",
    "    # Sauvegarde du nombre d'itérations\n",
    "    iterC=np.concatenate((iterC,np.array([nbiter])))\n",
    "\n",
    "# Affichage\n",
    "xx=np.log10((10*np.ones([condAmax]))**np.linspace(1,condAmax,num=condAmax))\n",
    "plt.plot(xx,iterC)\n",
    "plt.ylabel('Nb itérations')\n",
    "plt.xlabel('Conditionnement')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### I-2 Application de la méthode de Newton.\n",
    " \n",
    " **Question**: Implanter la méthode de Newton pour la minimisation d'une fonction quadratique. Vous utiliserez la méthode du gradient conjugué, déjà implantée, pour résoudre le système linéaire associé. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CG(x0,A,b,tol,itermax):\n",
    "    #Gradient conjugé\n",
    "    \n",
    "    x=x0\n",
    "    niter=0\n",
    "    r=b-np.dot(A,x)\n",
    "    d=r\n",
    "    nb=la.norm(b)\n",
    "    delta_new=np.dot(np.transpose(r),r)\n",
    "    iterf=np.array([math.sqrt(delta_new)])\n",
    "    convergence=0\n",
    "    \n",
    "    while convergence ==0:\n",
    "        niter=niter+1\n",
    "       \n",
    "        #pas optimal\n",
    "        q=np.dot(A,d)\n",
    "        den=np.dot(np.transpose(d),q)\n",
    "        alpha=delta_new/den\n",
    "        \n",
    "        #mise à jour\n",
    "        x=x+alpha*d\n",
    "        r=r-alpha*q\n",
    "    \n",
    "        delta_old=delta_new\n",
    "        delta_new=np.dot(np.transpose(r),r)\n",
    "       \n",
    "        #nouvelle direction\n",
    "        beta=delta_new/delta_old\n",
    "        d=r+beta*d\n",
    "        \n",
    "        #Test d'arrêt\n",
    "        if math.sqrt(delta_new) <= tol*(nb+10**(-14)):\n",
    "            convergence=1\n",
    "        elif niter == itermax:\n",
    "            convergence=2\n",
    "        \n",
    "        iterf=np.concatenate((iterf,np.array([math.sqrt(delta_new)])))\n",
    "        \n",
    "    #fin boucle\n",
    "    return x,iterf,niter,convergence    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction rélisant la minimisation par méthode de Newton \n",
    "# En sortie, la fonction devra renvoyer : \n",
    "#       x : vecteur contenant l'itéré optimisé.\n",
    "#       flag : un entier indiquant la raison d'arrêt de l'algorithme : 1 - convergence, \n",
    "#              2 - stagnation de la suite des itérés, 3 - nombre d'itérations maximum atteint.\n",
    "#       nbiter : nombre d'itérations réalisées.\n",
    "#       iterf : tableau contenant les valeurs de f au cours de la minimisation.\n",
    "#       iterCG : le nombre total d'itérations réalisées lors de sdifférents appels du gradient\n",
    "#                conjugué.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Appliquer la méthode de Newton au problème $(\\mathcal{P}_1)$. Vous afficherez le nombre d'itérations de la méthode de Newton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouvelle définition des paramètres du problème\n",
    "condA=3\n",
    "n=1000\n",
    "A,b=build_problem(n,condA)\n",
    "\n",
    "#Définition des paramètres de la méthode : x0, tolérances, nombre d'itérations,etc ..\n",
    "# TO DO\n",
    "\n",
    "# Résolution du problème d'optimisation\n",
    "# TO DO\n",
    "\n",
    "# Affichages \n",
    "#  x est le vecteur obtenu en sortie de minimisation.\n",
    "#  flag est l'entier indiquant la condition d'arrêt activée. \n",
    "#  fopt est la valeur optimale théorique.\n",
    "#  iterf est le tableau contenant les valeurs de f au cours de la minimisation.\n",
    "\n",
    "tmp=sla.solve(A,-b)\n",
    "fopt=0.5*np.dot(np.transpose(b),tmp)\n",
    "print(\"Valeur optimale théorique de f:\",fopt)\n",
    "print(\"Valeur à l'optinum numérique:\", fquad(x,A,b))\n",
    "print(\"Condition d'arrêt:\", flag)\n",
    "if flag > 1:\n",
    "    print(\"Norme du gradient initial:\",la.norm(gquad(x,A,b)))\n",
    "    print(\"Norme du gradient:\",la.norm(gquad(x,A,b)))\n",
    "\n",
    "plt.plot(iterf[:,0,0])\n",
    "plt.ylabel('f')\n",
    "plt.xlabel('Itérations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Illustrer l'impact du conditionnement de $A$ sur les performances de l'algorithme. Vous afficherez le nombre d'itérations de la méthode de Newton et le nombre d'itérations du gradient conjugué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intitialisation du problème\n",
    "x0=np.random.randn(n,1)\n",
    "param=[10**(-6),10**(-12),100000]\n",
    "iterC=np.array([])\n",
    "iterTCG=np.array([])\n",
    "condAmax=8\n",
    "\n",
    "# Boucle sur le nombre de conditionnement\n",
    "for condA in range(condAmax):\n",
    "    # TO DO\n",
    "    # Génération et résolution du problème d'optimisationA,b0=build_problem(n,condA)\n",
    "    \n",
    "    # Sauvegarde du nombre d'itérations de la méthdode de Newton et du gradient conjugué\n",
    "    iterC=np.concatenate((iterC,np.array([nbiter])))\n",
    "    iterTCG=np.concatenate((iterTCG,np.array([iterCG])))\n",
    "\n",
    "    \n",
    "# Affichage\n",
    "xx=np.log10((10*np.ones([condAmax]))**np.linspace(1,condAmax,num=condAmax))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(xx,iterC)\n",
    "plt.ylabel('Nb itérations Newton')\n",
    "plt.xlabel('Conditionnement')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(xx,iterTCG)\n",
    "plt.ylabel('Nb itérations CG')\n",
    "plt.xlabel('Conditionnement')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - Minimisation d'une fonction non-linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problèmes non-linéaires : fonction de Rosenbrock\n",
    "def f2(x):\n",
    "    # fonction de Rosenbrock de R^2 dans R\n",
    "    # x vecteur de taille 2\n",
    "    f=100*(x[1]-x[0]**2)**2+(1-x[0])**2\n",
    "    return f\n",
    "\n",
    "def g2(x):\n",
    "    # gradient de la fonction de Rosenbrock\n",
    "    # x vecteur de taille 2\n",
    "    g=np.zeros([2,1])\n",
    "    g[0]=-400.0*x[0]*(x[1]-x[0]**2)-2*(1-x[0])\n",
    "    g[1]=200.0*(x[1]-x[0]**2)\n",
    "    return g\n",
    "\n",
    "def h2(x):\n",
    "    # Hessienne de la fonction de Rosenbrock\n",
    "    # x vecteur de taille 2\n",
    "    H=np.zeros([2,2])\n",
    "    H[0,0]=1200*x[0]**2+2-400*x[1]\n",
    "    H[0,1]=-400*x[0]\n",
    "    H[1,0]=H[0,1]\n",
    "    H[1,1]=200.\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Implanter les stratégies de pas constant et de backtracking.  Implanter la méthode de descente de gradient avec choix de pas (vous pouvez utiliser l'algorithme  de descente de gradient développé pour la steepest descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Appliquer la méthode de descente avec les différentes stratégies de pas au problème $(\\mathcal{P}_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres du problème\n",
    "n=2\n",
    "# Minimum global\n",
    "xopt=np.ones([n,1])\n",
    "# Point de départ\n",
    "sigma_b=0.1\n",
    "x0=xopt+sigma_b*np.random.randn(n,1)\n",
    "\n",
    "#Définition des paramètres de la méthode : tolérances, nombre d'itérations,etc ..\n",
    "# TO DO\n",
    "\n",
    "# Résolution du problème d'optimisation\n",
    "# TO DO\n",
    "\n",
    "# Affichage\n",
    "#  x est le vecteur obtenu en sortie de minimisation.\n",
    "#  flag est l'entier indiquant la condition d'arrêt activée. \n",
    "#  iterf est le tableau contenant les valeurs de f au cours de la minimisation.\n",
    "\n",
    "print(\"Valeur optimale théorique de f:\",f2(xopt))\n",
    "print(\"Valeur à l'optinum numérique:\", f2(x))\n",
    "print(\"Condition d'arrêt:\", flag)\n",
    "if flag > 1:\n",
    "    print(\"Norme du gradient initial:\",la.norm(g2(x0)))\n",
    "    print(\"Norme du gradient:\",la.norm(g2(x)))\n",
    "\n",
    "    \n",
    "plt.plot(iterf[:,0])\n",
    "plt.ylabel('f')\n",
    "plt.xlabel('Itérations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Modifier le code l'algorithme Newton pour inclure la recherche de pas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# Fonction rélisant la minimisation par méthode de Newton avec stratégies de pas\n",
    "# En sortie, la fonction devra renvoyer : \n",
    "#       x : vecteur contenant l'itéré optimisé.\n",
    "#       flag : un entier indiquant la raison d'arrêt de l'algorithme : 1 - convergence, \n",
    "#              2 - stagnation de la suite des itérés, 3 - nombre d'itérations maximum atteint.\n",
    "#       nbiter : nombre d'itérations réalisées.\n",
    "#       iterf : tableau contenant les valeurs de f au cours de la minimisation.\n",
    "#       iterCG : le nombre total d'itérations réalisées lors de sdifférents appels du gradient\n",
    "#                conjugué.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Evaluer les performances de cette approche pour les différentes stratégies de pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres du problème\n",
    "n=2\n",
    "# Minimum global\n",
    "xopt=np.ones([n,1])\n",
    "# Point de départ\n",
    "sigma_b=1\n",
    "x0=xopt+sigma_b*np.random.randn(n,1)\n",
    "\n",
    "#Définition des paramètres de la méthode : tolérances, nombre d'itérations,etc ..\n",
    "# TO DO\n",
    "\n",
    "# Résolution du problème d'optimisation\n",
    "# TO DO\n",
    "\n",
    "# Affichage\n",
    "#  x est le vecteur obtenu en sortie de minimisation.\n",
    "#  flag est l'entier indiquant la condition d'arrêt activée. \n",
    "#  iterf est le tableau contenant les valeurs de f au cours de la minimisation.\n",
    "\n",
    "\n",
    "print(\"Valeur optimale théorique de f:\",f2(xopt))\n",
    "print(\"Valeur à l'optinum numérique:\", f2(x))\n",
    "print(\"Condition d'arrêt:\", flag)\n",
    "if flag > 1:\n",
    "    print(\"Norme du gradient initial:\",la.norm(g2(x0)))\n",
    "    print(\"Norme du gradient:\",la.norm(g2(x)))\n",
    "\n",
    "    \n",
    "plt.plot(iterf[:,0])\n",
    "plt.ylabel('f')\n",
    "plt.xlabel('Itérations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
