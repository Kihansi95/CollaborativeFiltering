{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Développement d'un algorithme en Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectif du Notebook\n",
    "Il s'agit de développer en Spark une méthode de gradient, dans le but de résoudre un problème de filtrage collaboratif, et de la comparer avec une méthode de la librairie MLIB. Ce Notebook a pour but le développement et la validation de l'approche, avant intégration et exploitation dans le cadre de l'infrastructure développée dans le projet. Pour information, de nombreuses versions de ce problème existent sur le web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position du problème\n",
    "Nous avons à notre disposition un RDD \"ratings\" du type (userID, movieID, rating). Les données sont fournies par le fichier `ratings.dat`, stockées  au format ci-joint :\n",
    "```\n",
    "UserID::MovieID::Rating::Timestamp\n",
    "```\n",
    "\n",
    "Ce RDD peut être stocké dans une matrice $R$ où l'on trouve \"rating\" à l'intersection de la ligne \"userID\" et de la colonne \"movieID\".\n",
    "Si la matrice $R$ est de taille $m \\times  n$, nous cherchons $P \\in R^{m,k}$ et $Q \\in R^{n,k}$ telles que $R \\approx \\hat{R} = PQ^T$.\n",
    "Pour cela on considère le problème\n",
    "$$ \\min_{P,Q} \\sum_{i,j : r_{ij} \\text{existe}}  \\ell_{i,j}(R,P,Q), $$\n",
    "où\n",
    "$$  \\ell_{i,j}(R,P,Q)= \\left(r_{ij} - q_{j}^{\\top}p_{i}\\right)^2 + \\lambda(|| p_{i} ||^{2}_2 + || q_{j} ||^2_2 )  $$ et $(p_i)_{1\\leq i\\leq m}$ et $(q_j)_{1\\leq j\\leq n}$ sont les lignes des matrices $P$ et $Q$ respectivement. Le paramètre $\\lambda\\geq 0$ est un paramètre de régularisation.\n",
    "\n",
    "Le problème que nous résolvons ici est un problème dit de \"filtrage collaboratif\", qui permet d'apporter une solution possible du  problème Netflix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Matrix Factorization, master=local[*]) created by __init__ at <ipython-input-1-8d8b3edeca7a>:16 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8d8b3edeca7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Matrix Factorization\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    306\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 308\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    309\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Matrix Factorization, master=local[*]) created by __init__ at <ipython-input-1-8d8b3edeca7a>:16 "
     ]
    }
   ],
   "source": [
    "# Librairies\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# Environnement Spark \n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# A modifier/commenter selon votre configuration.\n",
    "#import os\n",
    "#os.environ['PYSPARK_PYTHON'] = '/usr/local/insa/anaconda/bin/python3.6'\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "conf.setAppName(\"Matrix Factorization\")\n",
    "\n",
    "sc = SparkContext(conf = conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du RDD et premières statistiques sur le jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1000209 ratings from 6040 users on 3706 movies.\n",
      "\n",
      "We have 6040 users, 3952 movies and the rating matrix has 4.190221 percent of non-zero value.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Répertoire contenant le jeu de données\n",
    "movieLensHomeDir=\"data/\"\n",
    "\n",
    "# ratings est un RDD du type (userID, movieID, rating)\n",
    "def parseRating(line):\n",
    "    fields = line.split('::')\n",
    "    return int(fields[0]), int(fields[1]), float(fields[2])\n",
    "\n",
    "ratingsRDD = sc.textFile(movieLensHomeDir + \"ratings.dat\").map(parseRating).setName(\"ratings\").cache()\n",
    "\n",
    "# Calcul du nombre de ratings\n",
    "numRatings = ratingsRDD.count()\n",
    "# Calcul du nombre d'utilisateurs distincts\n",
    "numUsers = ratingsRDD.map(lambda r: r[0]).distinct().count()\n",
    "# Calcul du nombre de films distincts\n",
    "numMovies = ratingsRDD.map(lambda r: r[1]).distinct().count()\n",
    "print(\"We have %d ratings from %d users on %d movies.\\n\" % (numRatings, numUsers, numMovies))\n",
    "\n",
    "# Dimensions de la matrice R\n",
    "M = ratingsRDD.map(lambda r: r[0]).max()\n",
    "N = ratingsRDD.map(lambda r: r[1]).max()\n",
    "matrixSparsity = float(numRatings)/float(M*N)\n",
    "print(\"We have %d users, %d movies and the rating matrix has %f percent of non-zero value.\\n\" % (M, N, 100*matrixSparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser la routine ALS.train() de la librairie  [MLLib](http://spark.apache.org/docs/latest/ml-guide.html) et en évaluer la performance par un calcul de \" Mean Squared Error\" du  rating de prédiction.\n",
    "\n",
    "__Question 1__\n",
    "\n",
    "> Commenter les lignes de code suivantes en vous inspirant du code python http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html#collaborative-filtering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le choix du modèle de factorisation Train pour l'algorithme ALS comprend que les rantings sont explicites.\n",
    "Train a matrix factorization model given an RDD of ratings by users for a subset of products. The ratings matrix is approximated as the product of two lower-rank matrices of a given rank (number of features). To solve for these features, ALS is run iteratively with a configurable level of parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 0.5860886474868402\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "# Construction du modèle de recommendations depuis l'approche \"Alternating Least Squares\"\n",
    "rank = 10\n",
    "numIterations = 10\n",
    "\n",
    "# Paramètres de la méthode Alternating Least Squares (ALS)\n",
    "# ratings – RDD de Rating ou tuple (userID, productID, rating).\n",
    "# rank – Rang de la matrice modèle.\n",
    "# iterations – Nombre d'itérations. (default: 5)\n",
    "# lambda_ – Paramètre de régularisation. (default: 0.01)\n",
    "model = ALS.train(ratingsRDD, rank, iterations=numIterations, lambda_=0.02)\n",
    "\n",
    "# Evaluation du modèle sur le jeu de données complet\n",
    "testdata = ratingsRDD.map(lambda p: (p[0], p[1]))\n",
    "predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = ratingsRDD.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Algorithmes de descente de gradient\n",
    "\n",
    "Le but de cette section est  \n",
    "1. de calculer le gradient de la fonction,\n",
    "2. d'implémenter une méthode de gradient,\n",
    "3. de mesurer la précision de cette méthode\n",
    "\n",
    "__Question 2__\n",
    "\n",
    "> Séparer le jeu de données en un jeu d'apprentissage (70%) et un jeu de test, en utilisant la fonction randomsplit ( http://spark.apache.org/docs/2.0.0/api/python/pyspark.html )\n",
    "\n",
    "> Compléter la routine ci-dessous qui retourne le \"rating\" prédit. Créer un RDD contenant `(i,j,true rating,predicted rating)`. \n",
    "\n",
    "> Compléter la routine qui calcule le Mean Square Error (MSE) sur le jeu de données.\n",
    "\n",
    "> Tester ensuite la routine de MSE en vous donnant les matrices $P$ et $Q$ aléatoires (utiliser np.random.rand(M,K)) et calculer quelques \"ratings\" prédits. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation du jeu de données en un jeu d'apprentissage et un jeu de test\n",
    "# Taille du jeu d'apprentissage (en %) \n",
    "learningWeight = 0.7\n",
    "\n",
    "# Création des RDD \"apprentissage\" et \"test\" depuis la fonction randomsplit\n",
    "trainRDD, testRDD = ratingsRDD.randomSplit([0.7, 1-0.7], 17)\n",
    "\n",
    "# Calcul du rating préduit.\n",
    "def predictedRating(x, P, Q):\n",
    "    \"\"\" \n",
    "    This function computes predicted rating\n",
    "    Args:\n",
    "        x: tuple (UserID, MovieID, Rating)\n",
    "        P: user's features matrix (M by K)\n",
    "        Q: item's features matrix (N by K)\n",
    "    Returns:\n",
    "        predicted rating: l \n",
    "    \"\"\"\n",
    "    return P[ x[0] ]*( Q[ x[1] ].T )\n",
    "\n",
    "# Calcul de l'erreur MSE \n",
    "def computeMSE(rdd, P, Q):\n",
    "    \"\"\" \n",
    "    This function computes Mean Square Error (MSE)\n",
    "    Args:\n",
    "        rdd: RDD(UserID, MovieID, Rating)\n",
    "        P: user's features matrix (M by K)\n",
    "        Q: item's features matrix (N by K)\n",
    "    Returns:\n",
    "        mse: mean square error \n",
    "    \"\"\"\n",
    "    \n",
    "    return rdd.map( lambda r: (r[2] - predictedRating(r, P, Q)**2 )).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training dataset: 700260\n",
      "Size of the testing dataset: 299949\n",
      "P_rand =  [[0.29434431 0.45519849 0.38869693 ... 0.98231049 0.09287847 0.83468673]\n",
      " [0.58716164 0.19714313 0.10629755 ... 0.30134151 0.86978068 0.12141646]\n",
      " [0.57268459 0.96891859 0.97875988 ... 0.2281618  0.27004955 0.36922776]\n",
      " ...\n",
      " [0.17051875 0.49080502 0.74858581 ... 0.1726588  0.00147967 0.30764844]\n",
      " [0.01345534 0.95674734 0.02013415 ... 0.83523348 0.96152632 0.7534635 ]\n",
      " [0.87171726 0.1343663  0.5765171  ... 0.78675621 0.03837288 0.98462716]]\n",
      "Q_rand =  [[0.85600988 0.45574302 0.16986855 ... 0.18741667 0.58450158 0.96763118]\n",
      " [0.96366611 0.75494981 0.4866439  ... 0.62143531 0.49304449 0.67253757]\n",
      " [0.77458822 0.00981078 0.08383143 ... 0.80904682 0.81166996 0.06074864]\n",
      " ...\n",
      " [0.91061439 0.25765098 0.78818942 ... 0.80378796 0.64453569 0.22577609]\n",
      " [0.55887457 0.98601913 0.99358844 ... 0.57423987 0.86518383 0.70473214]\n",
      " [0.19337577 0.94040866 0.19955696 ... 0.30124892 0.52428697 0.32583332]]\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 206.0 failed 1 times, most recent failure: Lost task 1.0 in stage 206.0 (TID 169, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 230, in main\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 225, in process\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\", line 370, in func\n    return f(iterator)\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\", line 1083, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\statcounter.py\", line 42, in __init__\n    for v in values:\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-16-b9b3252da728>\", line 33, in <lambda>\n  File \"<ipython-input-16-b9b3252da728>\", line 19, in predictedRating\nIndexError: index 3952 is out of bounds for axis 0 with size 3952\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 230, in main\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 225, in process\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\", line 370, in func\n    return f(iterator)\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\", line 1083, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\statcounter.py\", line 42, in __init__\n    for v in values:\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-16-b9b3252da728>\", line 33, in <lambda>\n  File \"<ipython-input-16-b9b3252da728>\", line 19, in predictedRating\nIndexError: index 3952 is out of bounds for axis 0 with size 3952\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-2eb9d98db526>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Calcul et affichage de l'erreur MSE pour ces matrices aléatoires\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#### TO DO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputeMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainRDD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP_rand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ_rand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MSE test = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-b9b3252da728>\u001b[0m in \u001b[0;36mcomputeMSE\u001b[1;34m(rdd, P, Q)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \"\"\"\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpredictedRating\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1218\u001b[0m         \u001b[1;36m2.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         \"\"\"\n\u001b[1;32m-> 1220\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvariance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mstats\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mleft_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmergeStats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright_counter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStatCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mredFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhistogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    832\u001b[0m         \"\"\"\n\u001b[0;32m    833\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 834\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    835\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 206.0 failed 1 times, most recent failure: Lost task 1.0 in stage 206.0 (TID 169, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 230, in main\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 225, in process\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\", line 370, in func\n    return f(iterator)\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\", line 1083, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\statcounter.py\", line 42, in __init__\n    for v in values:\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-16-b9b3252da728>\", line 33, in <lambda>\n  File \"<ipython-input-16-b9b3252da728>\", line 19, in predictedRating\nIndexError: index 3952 is out of bounds for axis 0 with size 3952\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 230, in main\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 225, in process\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\", line 370, in func\n    return f(iterator)\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\lib\\site-packages\\pyspark\\rdd.py\", line 1083, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\statcounter.py\", line 42, in __init__\n    for v in values:\n  File \"D:\\Anaconda3\\envs\\cert-big-data\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-16-b9b3252da728>\", line 33, in <lambda>\n  File \"<ipython-input-16-b9b3252da728>\", line 19, in predictedRating\nIndexError: index 3952 is out of bounds for axis 0 with size 3952\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# Tailles des jeux de données d'apprentissage et de tests.\n",
    "print(\"Size of the training dataset:\", trainRDD.count())\n",
    "print(\"Size of the testing dataset:\", testRDD.count())\n",
    "\n",
    "\n",
    "# Création de matrices aléatoires de dimension (M,K) et (N,K)\n",
    "K = 20 \n",
    "# TO DO\n",
    "P_rand = np.random.rand(M,K)\n",
    "Q_rand = np.random.rand(N,K)\n",
    "print('P_rand = ', P_rand)\n",
    "print('Q_rand = ', Q_rand)\n",
    "\n",
    "# Calcul et affichage de l'erreur MSE pour ces matrices aléatoires\n",
    "#### TO DO\n",
    "mse = computeMSE(trainRDD, P_rand, Q_rand)\n",
    "print(\"MSE test = \", mse)\n",
    "\n",
    "# Affichage de quelques ratings prédits depuis ces matrices\n",
    "#### TO DO\n",
    "for i in range(3):\n",
    "    print('predict[', i, '] = ', predictedRating(trainRDD[i], P_rand, Q_rand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Question 3__\n",
    "\n",
    "> Donner la formule des dérivées des fonctions $\\ell_{i,j}$ selon $p_t$ et $q_s$ avec $1\\leq t\\leq m$ et $1\\leq s\\leq n$.\n",
    "\n",
    "> Commenter et compléter l'implantation de l'algorithme de gradient sur l'ensemble d'apprentissage. Prendre un pas égal à $\\gamma=0.001$ et arrêter sur un nombre maximum d'itérations. \n",
    "\n",
    "> Commenter les tracés de convergence et des indicateurs de qualité de la prévision en fonction de la dimension latente (rang de $P$ et $Q$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithem de descente de gradient pour la factorisation de matrices\n",
    "def GD(trainRDD, K=10, MAXITER=50, GAMMA=0.001, LAMBDA=0.05):\n",
    "    # Construction de la matrice R (creuse)\n",
    "    row=[]\n",
    "    col=[]\n",
    "    data=[]\n",
    "    for part in trainRDD.collect():\n",
    "        row.append(part[0]-1)\n",
    "        col.append(part[1]-1)\n",
    "        data.append(part[2])\n",
    "    R=sparse.csr_matrix((data, (row, col)))\n",
    "    \n",
    "    # Initialisation aléatoire des matrices P et Q\n",
    "    M,N = R.shape\n",
    "    P = np.random.rand(M,K)\n",
    "    Q = np.random.rand(N,K)\n",
    "    \n",
    "    # Calcul de l'erreur MSE initiale\n",
    "    mse=[]\n",
    "    mse_tmp = computeMSE(trainRDD, P, Q)\n",
    "    mse.append([0, mse_tmp])\n",
    "    print(\"epoch: \", str(0), \" - MSE: \", str(mse_tmp))\n",
    "    \n",
    "    # Boucle\n",
    "    nonzero = R.nonzero()\n",
    "    nbNonZero = R.nonzero()[0].size\n",
    "    I,J = nonzero[0], nonzero[1]\n",
    "    for epoch in range(MAXITER):\n",
    "        for i,j in zip(I,J):\n",
    "            # Mise à jour de P[i,:] et Q[j,:] par descente de gradient à pas fixe\n",
    "           #### TO DO\n",
    "        \n",
    "        # Calcul de l'erreur MSE courante, et sauvegarde dans le tableau mse \n",
    "        #### TO DO\n",
    "        \n",
    "    return P, Q, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de P, Q et de la mse\n",
    "P,Q,mse = GD(trainRDD, K=10, MAXITER=10, GAMMA=0.001, LAMBDA=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "# Affichage de l'erreur MSE\n",
    "#### TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4__\n",
    "\n",
    "> Calculer les ratings prédits par la solution de la méthode du gradient dans un RDD\n",
    "\n",
    "> Comparer sur le jeu de test les valeurs prédites aux ratings sur 5 échantillons aléatoires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul et affichage des ratings prédits\n",
    "#### TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
